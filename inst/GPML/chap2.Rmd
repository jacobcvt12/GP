---
title: "Chapter 2: Regression"
author: "Jacob Carey"
date: \today
output: pdf_document
---

# 2.1 Weight-space View

## Standard Linear Model

Typical linear regression models with Gaussian noise follow the parameterization
\begin{equation}
f(\mathbf{x}) = \mathbf{x}^T\mathbf{w}, \qquad y = f(\mathbf{x}) + \epsilon
\end{equation}

where $\mathbf{x}$ is the input, and $\mathbf{w}$ are the parameters of interest relating the input to the output $y$. We assume that $\epsilon$'s are IID Gaussian with mean 0, generally written as $\epsilon \sim \mathcal{N}(0, \sigma_n^2)$.

The *likelihood* of such a model is written as

\begin{equation}
p(\mathbf{y}|X, \mathbf{w}) = \mathcal{N}(X^T\mathbf{w}, \sigma_n^2 I)
\end{equation}

We typically use a non-informative prior
\begin{equation}
\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \Sigma_p)
\end{equation}

Combining the prior and likelihood, we arrive at the posterior

\begin{equation}
p(\mathbf{w}|X, \mathbf{y}) \sim \mathcal{N}(\mathbf{\bar{w}}=\frac{1}{\sigma_n^2}A^{-1}X\mathbf{y}, A^{-1})
\end{equation}

Where $A=\sigma_n^{-2}XX^T+\Sigma_p^{-1}$.

## Example 1

```{r, results="asis"}
# data
N <- 100
beta <- cbind(c(-4, 2))
sigma2.n <- 1
x <- seq(0, 4, len=N)
X <- rbind(rep(1, N), x)
epsilon <- rnorm(N, sd=sqrt(sigma2.n))
y <- t(X) %*% beta + epsilon

# prior
Sigma.p <- diag(100, nrow(beta))

# posterior 
A <- 1 / sigma2.n + X %*% t(X) + solve(Sigma.p)
w.bar <- 1 / sigma2.n * solve(A) %*% X %*% y
A.inv <- solve(A)

# 95% credible interval
se <- cbind(qnorm(0.975) * sqrt(diag(A.inv))) %*% c(-1, 1)
ci <- se + cbind(w.bar, w.bar)
rownames(ci) <- c("w[1]", "w[2]")
library(xtable)
options(xtable.comment = FALSE)
xtable(ci)
```

## Projections of Inputs into Feature Space

To extend the Bayesian linear model further, we can project a feature $x$ using the space of powers $x: \phi(x) = (1, x, x^2, ...)^T$. Then we now define the function $f$ as

\begin{equation}
f(\mathbf{x})=\phi(\mathbf{x})^T\mathbf{w}
\end{equation}

Note that for $D$-dimensional input vector $\mathbf{x}$, $\phi$ maps to an $N$-dimensional output, where $N>>D$.

**Notational considerations**: $\Phi=\Phi(X)$ is the aggregation of columns of $\phi(\mathbf{x})$ and now $A=\sigma_n^{-2} \Phi \Phi^T + \Sigma_p^{-1}$. Additionally, $\phi_*=\phi(x_*)$ and $K=\Phi^T\Sigma_p\Phi$.

In such a case, the weights $w$ are less important, as it is difficult to make intuitive inference. Instead, we are interested in the posterior predictive distribution:

\begin{equation}
f_{*}|\mathbf{x}_{*},X, \mathbf{y} \sim \mathcal{N}(\frac{1}{\sigma_n^2}\phi(\mathbf{x}_{*})^TA^{-1}\Phi\mathbf{y}, \phi(\mathbf{x}_{*})^TA^{-1}\phi(\mathbf{x}_{*}))
\end{equation}

Note that this parameterization requires the inversion of $A$, and $N\times N$ matrix, where $N$ may be large. Alternatively, we can rewrite this formulation as 
\begin{equation}
f_{*}|\mathbf{x}_{*},X, \mathbf{y} \sim \mathcal{N}(\phi_*^T\Sigma_p\Phi(K+\sigma_n^2I)^{-1}\mathbf{y},\phi_*^T\Sigma_p\phi_*-\phi_*^T\Sigma_p\Phi(K+\sigma_n^2I)^{-1}\Phi^T\Sigma_p\phi_*)
\end{equation}

which involes inversion of smaller matrices.

Since the feature space always enters in a similar fashion we define the *covariance function* or *kernel* $k(\cdot, \cdot)$ parameterized as $k(\mathbf{x}, \mathbf{x}')=\phi(\mathbf{x})^T\phi(\mathbf{x}')$.


# 2.2 Function-space View

# 2.3 Varying the Hyperparameters

# 2.4 Decision Theory for Regression

# 2.5 An Example Application

```{r data}
# read in matrices
library(R.matlab)
sarcos.inv <- readMat("data/sarcos_inv.mat")$sarcos.inv
sarcos.inv.test <- readMat("data/sarcos_inv_test.mat")$sarcos.inv.test
```

# 2.6 Smoothing, Weight Functions and Equivalent Kernels

# 2.7 Incorporating Explicit Basis Functions

# Exercises
