---
title: "Chapter 2: Regression"
author: "Jacob Carey"
date: \today
output: pdf_document
---

# 2.1 Weight-space View

Typical linear regression models with Gaussian noise follow the parameterization
\begin{equation}
f(\mathbf{x}) = \mathbf{x}^T\mathbf{w}, \qquad y = f(\mathbf{x}) + \epsilon
\end{equation}

where $\mathbf{x}$ is the input, and $\mathbf{w}$ are the parameters of interest relating the input to the output $y$. We assume that $\epsilon$'s are IID Gaussian with mean 0, generally written as $\epsilon \sim \mathcal{N}(0, \sigma_n^2)$.

The *likelihood* of such a model is written as

\begin{equation}
p(\mathbf{y}|X, \mathbf{w}) = \mathcal{N}(X^T\mathbf{w}, \sigma_n^2 I)
\end{equation}

We typically use a non-informative prior
\begin{equation}
\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \Sigma_p)
\end{equation}

Combining the prior and likelihood, we arrive at the posterior

\begin{equation}
p(\mathbf{w}|X, \mathbf{y}) \sim \mathcal{N}(\mathbf{\bar{w}}=\frac{1}{\sigma_n^2}A^{-1}X\mathbf{y}, A^{-1})
\end{equation}

Where $A=\sigma_n^{-2}XX^T+\Sigma_p^{-1}$.

# 2.2 Function-space View

# 2.3 Varying the Hyperparameters

# 2.4 Decision Theory for Regression

# 2.5 An Example Application

```{r data}
# read in matrices
library(R.matlab)
sarcos.inv <- readMat("data/sarcos_inv.mat")$sarcos.inv
sarcos.inv.test <- readMat("data/sarcos_inv_test.mat")$sarcos.inv.test
```

# 2.6 Smoothing, Weight Functions and Equivalent Kernels

# 2.7 Incorporating Explicit Basis Functions

# Exercises
