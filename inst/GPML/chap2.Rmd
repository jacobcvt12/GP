---
title: "Chapter 2: Regression"
author: "Jacob Carey"
date: \today
output: pdf_document
---

# 2.1 Weight-space View

## Standard Linear Model

Typical linear regression models with Gaussian noise follow the parameterization
\begin{equation}
f(\mathbf{x}) = \mathbf{x}^T\mathbf{w}, \qquad y = f(\mathbf{x}) + \epsilon
\end{equation}

where $\mathbf{x}$ is the input, and $\mathbf{w}$ are the parameters of interest relating the input to the output $y$. We assume that $\epsilon$'s are IID Gaussian with mean 0, generally written as $\epsilon \sim \mathcal{N}(0, \sigma_n^2)$.

The *likelihood* of such a model is written as

\begin{equation}
p(\mathbf{y}|X, \mathbf{w}) = \mathcal{N}(X^T\mathbf{w}, \sigma_n^2 I)
\end{equation}

We typically use a non-informative prior
\begin{equation}
\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \Sigma_p)
\end{equation}

Combining the prior and likelihood, we arrive at the posterior

\begin{equation}
p(\mathbf{w}|X, \mathbf{y}) \sim \mathcal{N}(\mathbf{\bar{w}}=\frac{1}{\sigma_n^2}A^{-1}X\mathbf{y}, A^{-1})
\end{equation}

Where $A=\sigma_n^{-2}XX^T+\Sigma_p^{-1}$.

## Example 1

```{r, results="asis"}
# data
N <- 100
beta <- cbind(c(-4, 2))
sigma2.n <- 1
x <- seq(0, 4, len=N)
X <- rbind(rep(1, N), x)
epsilon <- rnorm(N, sd=sqrt(sigma2.n))
y <- t(X) %*% beta + epsilon

# prior
Sigma.p <- diag(100, nrow(beta))

# posterior 
A <- 1 / sigma2.n + X %*% t(X) + solve(Sigma.p)
w.bar <- 1 / sigma2.n * solve(A) %*% X %*% y
A.inv <- solve(A)

# 95% credible interval
se <- cbind(qnorm(0.975) * sqrt(diag(A.inv))) %*% c(-1, 1)
ci <- se + cbind(w.bar, w.bar)
rownames(ci) <- c("w[1]", "w[2]")
library(xtable)
options(xtable.comment = FALSE)
xtable(ci)
```

## Projections of Inputs into Feature Space

# 2.2 Function-space View

# 2.3 Varying the Hyperparameters

# 2.4 Decision Theory for Regression

# 2.5 An Example Application

```{r data}
# read in matrices
library(R.matlab)
sarcos.inv <- readMat("data/sarcos_inv.mat")$sarcos.inv
sarcos.inv.test <- readMat("data/sarcos_inv_test.mat")$sarcos.inv.test
```

# 2.6 Smoothing, Weight Functions and Equivalent Kernels

# 2.7 Incorporating Explicit Basis Functions

# Exercises
