---
title: "Chapter 3: Classification"
author: "Jacob Carey"
date: \today
output: pdf_document
---

```{r}
# libraries
library(ggplot2)
library(mvtnorm)
library(fields)
theme_set(theme_classic())

# generate binomial data for linear model
set.seed(90210)
N <- 100
beta <- 2.5

x.star <- seq(-10, 10, len=N ^ 2)
x <- sample(x.star, N)
z <- beta * x
p <- 1 / (1 + exp(-z))
y <- rbinom(N, 1, p)

data <- data.frame(x=x, p=p, y=y)

ggplot(data, aes(x, y)) +
    geom_point() +
    geom_line(aes(x=x, y=p))

# frequentist estimate
model <- glm(y ~ x - 1, family="binomial")

# now generate binomial data for nonlinear model
z <- sin(x)
p <- 1 / (1 + exp(-z))
y <- rbinom(N, 1, p)

data <- data.frame(x=x, p=p, y=y)

ggplot(data, aes(x, y)) +
    geom_point() +
    geom_line(aes(x=x, y=p))

model <- glm(y ~ x - 1, family="binomial") # doesn't work

# GP classification
K <- function(X.p, X.q=NULL) {
    sigma <- exp(-0.5 * rdist(X.p, X.q) ^ 2)
    return(sigma)
}

f <- rep(0, length(x))   # initialize to 0 
K <- K(x)                # calcualte K
eps <- 0.1               # convergence criterion
y.svm <- y
y.svm[y.svm == 0] <- -1  # svm notation

# initialize objective function to high value
obj <- 1000

# repeat until convergence
while (obj > eps) {
    # calculate probability using logistic function
    pi <- as.vector(1 / (1 + exp(-f)))
    # construct W (- Hessian of logistic log likelihood)
    W <- diag(pi * (1 - pi))
    # take square root of W for calculating B
    # can just use sqrt since W is diagonal
    W.sqrt <- sqrt(W) 
    B <- diag(1, nrow(K)) + W.sqrt %*% K %*% W.sqrt
    L <- chol(B)
    b <- W %*% f + cbind(y - pi)
    a <- b - solve(W.sqrt %*% t(L), solve(L, W.sqrt %*% K %*% b))

    # update f
    f <- K %*% a

    # recalculate objective function
    obj <- -0.5 * t(a) %*% f - log(1 + exp(-y.svm %*% f))
}
```
